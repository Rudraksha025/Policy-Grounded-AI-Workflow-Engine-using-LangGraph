# ðŸ¦ Compliance AI System  
### Policy-Enforced Loan Review Engine with Human-in-the-Loop

---

## 1. What this project is (and what it is not)

This project implements a **policy-enforced AI compliance workflow**, designed for regulated decision-making scenarios such as loan approvals.

It is **not**:
- a chatbot
- a simple LLM wrapper
- a prompt-engineering demo

It **is**:
- a deterministic AI workflow
- with pre-LLM intent detection
- policy-grounded reasoning (RAG)
- automatic validation and self-correction
- guaranteed human escalation for unsafe intent
- immutable decisions
- full audit logging
- SLA monitoring
- role-separated user and reviewer interfaces

The system is designed to answer one question reliably:

> *When should AI decide, when should it correct itself, and when must a human intervene?*

---

## 2. High-level system flow

At a conceptual level, the system works like this:

User Input
â†“
Intent Detection (Pre-LLM)
â†“
AI Generation (Policy-Grounded)
â†“
Validation (Rules + Behavior)
â†“
Decision Routing
â”œâ”€ Auto-Finalize (APPROVED)
â””â”€ Human Escalation (PENDING_HUMAN)
â†“
Human Review
â†“
Final Approval



The key idea is that **not all inputs are equal**:
- Some are safe and automatable
- Some are fixable by refinement
- Some are dangerous and must never be auto-approved

---

## 3. Core design principles

This system follows five non-negotiable principles:

1. **Intent must be detected before generation**
2. **Policy overrides must never be auto-handled**
3. **AI decisions must be auditable**
4. **Approved decisions must be immutable**
5. **Humans intervene only when required**

Every file and function exists to enforce one of these principles.

---

## 4. Directory structure overview

.
â”œâ”€â”€ main.py # FastAPI app + LangGraph engine
â”œâ”€â”€ user_app.py # User Streamlit UI
â”œâ”€â”€ reviewer_app.py # Reviewer Streamlit UI
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ auth/
â”‚ â”‚ â””â”€â”€ basic_auth.py # Reviewer authentication
â”‚ â”‚
â”‚ â”œâ”€â”€ config/
â”‚ â”‚ â””â”€â”€ env_var.py # Environment variables
â”‚ â”‚
â”‚ â”œâ”€â”€ database/
â”‚ â”‚ â”œâ”€â”€ db.py # SQLAlchemy engine/session
â”‚ â”‚ â”œâ”€â”€ models.py # ComplianceRequest table
â”‚ â”‚ â”œâ”€â”€ events.py # ComplianceEvent audit table
â”‚ â”‚ â”œâ”€â”€ crud.py # Controlled DB operations
â”‚ â”‚ â””â”€â”€ init_db.py # DB bootstrap script
â”‚ â”‚
â”‚ â”œâ”€â”€ engine/
â”‚ â”‚ â””â”€â”€ graph_runner.py # run_compliance_review
â”‚ â”‚
â”‚ â”œâ”€â”€ nodes/
â”‚ â”‚ â”œâ”€â”€ generate_node.py
â”‚ â”‚ â”œâ”€â”€ validate_node.py
â”‚ â”‚ â”œâ”€â”€ refine_node.py
â”‚ â”‚ â”œâ”€â”€ decide_next.py
â”‚ â”‚ â””â”€â”€ human_review.py
â”‚ â”‚
â”‚ â”œâ”€â”€ policies/
â”‚ â”‚ â””â”€â”€ loan_policy.txt # Ground truth policy
â”‚ â”‚
â”‚ â”œâ”€â”€ schema/
â”‚ â”‚ â”œâ”€â”€ state.py # ReviewState (graph contract)
â”‚ â”‚ â””â”€â”€ pending_review.py # API request schema
â”‚ â”‚
â”‚ â”œâ”€â”€ shared/
â”‚ â”‚ â””â”€â”€ utils.py # LLM, RAG, validation, intent detection
â”‚ â”‚
â”‚ â”œâ”€â”€ views/
â”‚ â”‚ â”œâ”€â”€ review.py # Submit request API
â”‚ â”‚ â”œâ”€â”€ result.py # Poll result API
â”‚ â”‚ â”œâ”€â”€ pending_reviews.py # Reviewer queue API
â”‚ â”‚ â””â”€â”€ human_approve.py # Approve endpoint
â”‚ â”‚
â”‚ â””â”€â”€ monitor/
â”‚ â””â”€â”€ sla_monitor.py # SLA timeout checker


---

## 5. The data model (source of truth)

### 5.1 compliance_requests

This table stores the **entire lifecycle** of a compliance decision.

Fields:
- `request_id` â€“ immutable identifier
- `user_input` â€“ original user text
- `ai_draft` â€“ AI-generated or placeholder output
- `final_output` â€“ approved decision (nullable)
- `status` â€“ workflow state
- `created_at` â€“ timestamp

This table answers:
> â€œWhat decision exists right now?â€

---

### 5.2 compliance_events (audit trail)

This table logs **every status transition**.

Fields:
- `id`
- `request_id`
- `old_status`
- `new_status`
- `timestamp`

This table answers:
> â€œWhat happened, when, and in what order?â€

No compliance system is valid without this.

---

## 6. Intent detection (the most critical layer)

Before the LLM is ever called, the system runs:

```python
detect_intent_conflict(user_input)
This detects whether the user is attempting to:
- ignore policy
- override rules
- coerce approval

The result is a boolean flag:
intent_override = True | False

This flag is:
- computed once
- immutable
- passed into the LangGraph state
- never re-evaluated later

This prevents jailbreaks caused by LLM rewriting.


## 7 ReviewState (LangGraph contract)
All nodes operate on a shared, explicit state:

class ReviewState(TypedDict):
    request_id: str
    input: str
    intent_override: bool
    output: str
    validated: bool
    violations: List[str]
    retries: int
    requires_human: bool
If a field is not declared here, it does not exist in the graph.

This guarantees determinism.

## 8. LangGraph Nodes â€” Step-by-Step Execution

The decision-making logic is implemented using LangGraph.  
Each node has a **single responsibility** and cannot bypass others.

The graph is deterministic: the same input always produces the same routing behavior.

---

### 8.1 generate_node â€” Policy-Grounded Generation

**Purpose**
- Generate a compliance explanation strictly based on policy
- OR immediately escalate if the user intent is unsafe

**Execution Logic**
1. If `intent_override == True`
   - The LLM is NOT called
   - A neutral placeholder message is created
   - Status is set to `PENDING_HUMAN`
   - Execution stops

2. If `intent_override == False`
   - Policy text is retrieved using RAG
   - The LLM generates a compliance explanation
   - Draft is stored in the database
   - Status becomes `PENDING_VALIDATION`

This ensures the AI never reasons about policy overrides.

---

### 8.2 validate_node â€” Compliance Validation

**Purpose**
- Decide whether the AI output is compliant and final

**Validation Checks**
- Weak or uncertain language
- Illegal approvals
- Policy violations
- Intent override flag

**Rules**
- If **any violation exists** â†’ output is NOT valid
- If **no violations exist** â†’ output is final

A strict rejection is considered a valid and complete decision.

---

### 8.3 refine_node â€” Language Correction Loop

**Purpose**
- Improve clarity and authority of the explanation
- Never change the decision outcome

**Behavior**
- Rewrites the explanation using stricter language
- Increments retry counter
- Does not interact with policy or intent logic

Refinement exists only to fix wording, not decisions.

---

### 8.4 decide_next â€” Deterministic Routing

**Purpose**
- Decide the next step in the workflow

**Routing Rules**
- If `validated == True` â†’ END
- If retry limit exceeded â†’ `human_review`
- Otherwise â†’ `refine`

This node performs no reasoning. It only routes execution.

---

### 8.5 human_review_node â€” Human Escalation

**Purpose**
- Persist escalation state
- End AI-driven execution

**Behavior**
- Stores the case as `PENDING_HUMAN`
- Marks the request as requiring human review
- Ends the LangGraph execution

After this node, only a human can finalize the decision.

---

## 9. Human-in-the-Loop Workflow

Cases are escalated to humans only when:
- The user attempts to override policy
- The AI cannot produce a compliant output after retries

### Human Reviewer Capabilities
- View pending cases
- Inspect original input and AI draft
- Edit the final decision
- Approve once

### Constraints
- Approved records cannot be modified
- All approvals are logged
- Authentication is mandatory

This guarantees accountability.

---

## 10. SLA Monitoring

A background monitor continuously checks for requests stuck in:

PENDING_HUMAN > 24hours

This enables:
- SLA tracking
- Operational alerts
- Escalation handling

Compliance systems must monitor unresolved cases.

---

## 11. User Interface Separation

### User Portal
- Submit compliance requests
- Poll request status
- View final decision
- No access to drafts or other users

### Reviewer Portal
- Authenticated access only
- View pending human reviews
- Approve final decisions
- No submission capability

Role separation prevents misuse and confusion.

---

## 12. Authentication

Reviewer endpoints are protected using HTTP Basic Authentication.

Without valid credentials:
- Pending reviews cannot be accessed
- Approvals are rejected

This prevents anonymous or unauthorized approvals.

---

## 13. Why This Architecture Matters

This system clearly defines:
- When AI is allowed to decide
- When AI must stop
- When humans must intervene
- How every decision can be audited later

This is the foundation of regulated AI systems.

---

## 14. Running the System

### Backend
uvicorn main:app --reload

### User Interface
streamlit run user_app.py

### Reviewer Interface
streamlit run reviewer_app.py

